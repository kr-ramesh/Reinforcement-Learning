# -*- coding: utf-8 -*-
"""DQNWithDoom.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tq3AnozwtJA2G6CmfysgtQn9tBMC_JcV
"""



# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # Install deps from 
# # https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux
# 
# apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \
# nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \
# libopenal-dev timidity libwildmidi-dev unzip
# 
# # Boost libraries
# apt-get install libboost-all-dev
# 
# # Lua binding dependencies
# apt-get install liblua5.1-dev

!pip install vizdoom

!pip3 install --upgrade --force-reinstall tensorflow-gpu==1.15.0

#import necessary dependencies
import tensorflow as tf
import numpy as np
from vizdoom import *
from collections import deque
from skimage import transform
import random
import time
import matplotlib.pyplot as plt 

import warnings
warnings.filterwarnings('ignore')

#creating the environment for doom

def create():
  game=DoomGame()
  game.set_window_visible(False)
  game.load_config('/usr/local/lib/python3.6/dist-packages/vizdoom/scenarios/basic.wad')
  game.set_doom_scenario_path('/usr/local/lib/python3.6/dist-packages/vizdoom/scenarios/basic.cfg')
  game.init()
  
  #action space
  right=[0,0,1]
  shoot=[0,1,0]
  left=[1,0,0]
  action_space=[left, shoot, right]
  return action_space,game

def test_env():
  game=DoomGame()
  game.set_window_visible(False)
  game.load_config('/usr/local/lib/python3.6/dist-packages/vizdoom/scenarios/basic.wad')
  game.set_doom_scenario_path('/usr/local/lib/python3.6/dist-packages/vizdoom/scenarios/basic.cfg')
  game.init()

  right=[0,0,1]
  left=[1,0,0]
  shoot=[0,1,0]
  action_space=[left,shoot,right]
  total_ep=10
  for e in range(1000):
    game.new_episode()
    while not game.is_episode_finished():
      state=game.get_state()
      action=random.choice(action_space)
      info=state.game_variables
      image=state.screen_buffer
      print("Action chosen: ", action)
      reward=game.make_action(action)
      print("Reward obtained: ", reward)
      time.sleep(0.05)
    game.close()

a_space, game=create()

def rgb2gray(rgb):

    r, g, b = rgb[0,:,:], rgb[1,:,:], rgb[2,:,:]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b

    return gray

def preproc(frame):
    # Greyscale frame already done in our vizdoom config
    # x = np.mean(frame,-1)
    frame=rgb2gray(frame)
    #print(frame.shape)
    # Crop the screen (remove the roof because it contains no information)
    cropped_frame = frame[30:-10,30:-30]
    
    # Normalize Pixel Values
    normalized_frame = cropped_frame/255.0
    
    # Resize
    preprocessed_frame = transform.resize(normalized_frame, [84,84])
    
    return preprocessed_frame

size=4
stacked=deque([np.zeros((84,84), dtype=np.int) for x in range(size)] ,maxlen=size)

def stacking(frame,test_if_new,stacked):
  pframe=preproc(frame)
  if test_if_new:
    stacked=deque([np.zeros((84,84), dtype=np.int) for x in range(size)],maxlen=size)
    for i in range(4):
      stacked.append(pframe)
    new_stacked=np.stack(stacked, axis=2)
  else:
    stacked.append(pframe)
    new_stacked=np.stack(stacked, axis=2)
  return new_stacked,stacked

state_dim=[84,84,4]
action_space_size=game.get_available_buttons_size()
alpha=0.0002
stepcount=100
total_eps=500
batchsize=64

memory_size=1000000
m_len_init=batchsize
training=True
renderep=False

gamma=0.95
max_epsilon=1.0
min_epsilon=0.01
decay=0.0001

class DQN:
  def __init__(self,state_dim,action_space_size,alpha, name='DQN'):
    self.state_dim=state_dim
    self.action_space_size=action_space_size
    self.alpha=alpha
    with tf.variable_scope(name):
      self.actions=tf.placeholder(tf.float32, [None,3], name='actions')
      self.inputs=tf.placeholder(tf.float32, [None, *state_dim], name='inputs')
      self.targetQ=tf.placeholder(tf.float32, [None], name='targetQ')

      #first layer of the CNN
      self.conv1=tf.layers.conv2d(inputs=self.inputs,filters=32, kernel_size=[8,8], strides=[4,4], padding="VALID", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),name="conv1")
      self.batch_norm1=tf.layers.batch_normalization(self.conv1,epsilon=1e-5, training=True, name="batch_norm_1")
      self.conv1output=tf.nn.elu(self.batch_norm1, name="conv1_output")

      #second layer of the CNN
      self.conv2=tf.layers.conv2d(inputs=self.conv1output, filters=64, kernel_size=[4,4], strides=[2,2], padding="VALID", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),name="conv2")
      self.batch_norm2=tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=True, name="batch_norm2")
      self.conv2output=tf.nn.elu(self.batch_norm2, name="conv2_output")

      #third layer of the CNN 
      self.conv3=tf.layers.conv2d(inputs=self.conv2output, filters=128, kernel_size=[2,2], strides=[4,4], padding="VALID", kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),name="conv3")
      self.batch_norm3=tf.layers.batch_normalization(self.conv3, epsilon=1e-5,training=True, name="batch_norm3")
      self.conv3output=tf.nn.elu(self.batch_norm3, name="conv3_output")

      self.flatten=tf.layers.flatten(self.conv3output)

      self.fcn1=tf.layers.dense(inputs=self.flatten, units=512, activation=tf.nn.elu, kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name="fully_connected_layer1" )
      self.output_func=tf.layers.dense(inputs=self.fcn1, kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), units=3, activation=None)

      self.Q=tf.reduce_sum(tf.multiply(self.output_func,self.actions),axis=1)
      self.loss_function=tf.reduce_mean(tf.square(self.targetQ-self.Q))

      self.optimize=tf.train.RMSPropOptimizer(self.alpha).minimize(self.loss_function)

tf.reset_default_graph()
DQN= DQN(state_dim, action_space_size,alpha)

class memory_buffer():
  def __init__(self,max_size):
    self.buffer=deque(maxlen=max_size)
  def add(self, exp):
    self.buffer.append(exp)
  def sample_action(self, batchsize):
    buff_length=len(self.buffer)
    index= np.random.choice(np.arange(buff_length), size=batchsize,replace=False)
    return [self.buffer[i] for i in index]

memory=memory_buffer(max_size=memory_size)

game.new_episode()

for i in range(m_len_init):
  if i==0:
    state=game.get_state().screen_buffer
    #print(state.shape)
    state, stacked=stacking(state,True,stacked)
  
  action=random.choice(a_space)
  reward=game.make_action(action)
  print(reward)
  done=game.is_episode_finished()

  if done:
    next_state=np.zeros(state.shape)
    memory.add((state,action,next_state,reward,done))

    game.new_episode()
    state=game.get_state().screen_buffer
    state, stacked= stacking(state,True,stacked)
  
  else:
    next_state=game.get_state().screen_buffer
    next_state, stacked= stacking(next_state,False,stacked)

    memory.add((state,action,next_state,reward,done))
    state=next_state

#Setting up Tensorboard
writer = tf.summary.FileWriter("/tensorboard/dqn/1")

tf.summary.scalar("Loss:", DQN.loss_function)
write_op = tf.summary.merge_all()



epsilon=1.0
def pred_action(max_epsilon,min_epsilon,decay,decay_step,state,a_space):
  exploration=np.random.rand()
  epsilon=min_epsilon+ (max_epsilon-min_epsilon)*np.exp(-decay*decay_step)
  if(epsilon>exploration):
    x=random.choice(a_space)
  else:
    Q= sess.run(DQN.output_func, feed_dict = {DQN.inputs: state.reshape((1, *state.shape))})
    actions=np.argmax(Q)
    x=a_space[int(actions)]
  return x, epsilon

saver=tf.train.Saver()

if training==True:
  with tf.Session() as sess:
    sess.run((tf.global_variables_initializer()))
    decay_step=0
    #initialize the game
    game.init()
    for ep in range(total_eps):
      print(ep)
      game.new_episode()
      total_reward=0
      state=game.get_state().screen_buffer
      state,stacked=stacking(state,True,stacked)
      steps=0
      while steps<stepcount:
        steps+=1
        decay_step+=1
        action,epsilon=pred_action(max_epsilon,min_epsilon,decay,decay_step,state,a_space)
        reward=game.make_action(action)
        print(reward)
        total_reward+=reward
        done=game.is_episode_finished()
        if done:
          nextstate=np.zeros((84,84))
          nextstate,stacked=stacking(nextstate,False,stacked)
          memory.add((state,action,nextstate,reward,done))
          print("Total reward", total_reward, "\nLoss",loss,"\nEpsilon:",epsilon)
          steps=stepcount
        else:
          nextstate=game.get_state().screen_buffer
          nextstate,stacked=stacking(nextstate,False,stacked)
          memory.add((state,action,nextstate,reward,done))
          state=nextstate

        batch=memory.sample_action(batchsize)
        #print(len(batch))
        #for each in batch:
          #print(each[0].shape)
        #print("END")
        m_state= np.array([each[0] for each in batch],ndmin=4)
        m_actions = np.array([each[1] for each in batch])
        m_reward= np.array([each[3] for each in batch]) 
        m_next_states = np.array([each[2] for each in batch], ndmin=4)
        m_done = np.array([each[4] for each in batch])
        #print(m_reward)
        tQ=[]

        Q_next_state = sess.run(DQN.output_func, feed_dict = {DQN.inputs: m_next_states})

        for i in range(0,len(batch)):
          ex=m_done[i]
          if ex==True:
            tQ.append(m_reward[i])
          else:
            calc=m_reward[i]+(np.max(Q_next_state[i])*gamma)
            tQ.append(calc)

        target_Q = np.array([each for each in tQ])
        #print(target_Q.shape)
        
        loss, _ =sess.run([DQN.loss_function,DQN.optimize], feed_dict={DQN.inputs:m_state,DQN.actions:m_actions,DQN.targetQ:target_Q})
        summary = sess.run(write_op, feed_dict={DQN.inputs:m_state,DQN.actions:m_actions,DQN.targetQ:target_Q})
        writer.add_summary(summary,ep)
        writer.flush()
      if (ep%10==0):
        save_path = saver.save(sess, "model.ckpt")
        print("Model Saved")
      print("Total reward", total_reward, "\nLoss",loss,"\nEpsilon:",epsilon)

with tf.Session() as sess:    
    game, possible_actions = create_environment()
    
    totalScore = 0
    
    # Load the model
    saver.restore(sess, "model.ckpt")
    game.init()
    for i in range(1):
        
        done = False
        
        game.new_episode()
        
        state = game.get_state().screen_buffer
        state, stacked_frames = stack_frames(stacked_frames, state, True)
            
        while not game.is_episode_finished():
            # Take the biggest Q value (= the best action)
            Qs = sess.run(DQNetwork.output_func, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})
            
            # Take the biggest Q value (= the best action)
            choice = np.argmax(Qs)
            action = possible_actions[int(choice)]
            
            game.make_action(action)
            done = game.is_episode_finished()
            score = game.get_total_reward()
            
            if done:
                break  
                
            else:
                print("else")
                next_state = game.get_state().screen_buffer
                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)
                state = next_state
                
        score = game.get_total_reward()
        print("Score: ", score)
    game.close()